<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conversation Translator</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    <link rel="stylesheet" href="/static/style.css">
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Conversation Translator</h1>
            <button id="record-btn" class="record-btn">
                <i id="record-icon" class="fas fa-microphone"></i>
            </button>
        </div>
        
        <!-- This is the new chat history container -->
        <div id="chat-container" class="chat-container">
            <!-- Messages will be added here dynamically -->
            <div class="status-message">Press the microphone and start speaking to begin your conversation.</div>
        </div>

    </div>
    <script src="/static/script.js"
    
    document.addEventListener('DOMContentLoaded', () => {
        // This is the URL of your deployed Python app
        const TRANSLATE_API_URL = 'https://yptox.pythonanywhere.com/translate';
    
        // --- Speech Recognition Setup ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            CABLES.patch.setVariable('liveFinalSubtitle', "Error: Browser doesn't support speech recognition.");
            return;
        }
        
        const recognition = new SpeechRecognition();
        recognition.continuous = true; // Keep listening even after a pause
        recognition.interimResults = true; // Get live, non-final results
    
        let finalTranscript = '';
    
        // --- THIS IS THE MAIN LOGIC LOOP ---
        recognition.onresult = (event) => {
            let interimTranscript = '';
            
            // Loop through all the results from the API
            for (let i = event.resultIndex; i < event.results.length; ++i) {
                // If the result is final, it means the user has paused.
                if (event.results[i].isFinal) {
                    const finalChunk = event.results[i][0].transcript.trim();
                    // Send the final chunk to our backend for translation
                    getFinalTranslation(finalChunk);
                    // Clear the final transcript to start the next sentence
                    finalTranscript = ''; 
                } else {
                    // Otherwise, it's an interim result.
                    interimTranscript += event.results[i][0].transcript;
                }
            }
            
            // *** UX IMPROVEMENT ***
            // Display the LIVE, UNTRANSLATED text immediately in the AR view.
            // This gives the user instant feedback that the mic is working.
            CABLES.patch.setVariable('liveUnsafeSubtitle', interimTranscript);
        };
    
        // Restart recognition if it stops for any reason
        recognition.onend = () => {
            console.log("Speech recognition ended, restarting...");
            recognition.start();
        };
    
        // --- Function to call your PythonAnywhere API ---
        async function getFinalTranslation(textChunk) {
            try {
                const response = await fetch(TRANSLATE_API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text: textChunk }),
                });
    
                if (!response.ok) {
                    throw new Error(`Server error: ${response.status}`);
                }
    
                const data = await response.json();
    
                if (data.error) {
                    CABLES.patch.setVariable('liveFinalSubtitle', `Error: ${data.error}`);
                } else {
                    // *** INTEGRATION POINT ***
                    // Update the AR display with the final, translated text
                    CABLES.patch.setVariable('liveFinalSubtitle', data.translated_text);
                    // Clear the interim text display
                    CABLES.patch.setVariable('liveUnsafeSubtitle', '');
                }
            } catch (error) {
                console.error('Translation fetch error:', error);
                CABLES.patch.setVariable('liveFinalSubtitle', 'Error: Could not connect.');
            }
        }
    
        // --- UI to Start ---
        // Hook this up to a button in your HTML or a trigger in Cables.gl
        const startButton = document.getElementById('start-ar-translation');
        startButton.onclick = () => {
            recognition.start();
            startButton.textContent = "Listening...";
            startButton.disabled = true;
            // Provide an initial message
            CABLES.patch.setVariable('liveFinalSubtitle', 'Listening...');
        };
    }


    </script>
</body>
</html>